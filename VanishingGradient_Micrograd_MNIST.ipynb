{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZYyfl8J41hkqmeE3MbD9/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewstheBuilder/ScratchNeuralNetworks/blob/main/VanishingGradient_Micrograd_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "I7VKMgtrIA4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install micrograd_andrews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mptXKSt6Httg",
        "outputId": "60658181-38c7-4a64-9fd0-eed5fb0d4ab7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: micrograd_andrews in /usr/local/lib/python3.10/dist-packages (0.1.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Micrograd imports\n",
        "from micrograd_andrews.engine import Value\n",
        "from micrograd_andrews.nn import Neuron, Layer, MLP"
      ],
      "metadata": {
        "id": "xR5VcbmJHvbs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# other imports\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "from keras.datasets import mnist\n",
        "import copy\n",
        "\n",
        "np.random.seed(1337)\n",
        "random.seed(1337)"
      ],
      "metadata": {
        "id": "a1Ny8OhKHwWN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and test data\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "# normalize to have training values for pixels between 0-1.\n",
        "train_X = train_X.astype('float32') / 255.0\n",
        "test_X = test_X.astype('float32') / 255.0\n",
        "\n",
        "# print('X_train: ' + str(train_X.shape))\n",
        "# print('Y_train: ' + str(train_y.shape))\n",
        "# print('X_test:  '  + str(test_X.shape))\n",
        "# print('Y_test:  '  + str(test_y.shape))\n",
        "\n",
        "# print('train_x[1] raw',train_X[1][5][10:21])\n",
        "# pyplot.imshow(train_X[0], cmap=pyplot.get_cmap('gray'))\n",
        "# pyplot.show()\n",
        "\n",
        "flattened_trainX = train_X.reshape(-1,28*28)\n",
        "print(np.mean(flattened_trainX[0:100], axis=1))\n",
        "# def findOne(x):\n",
        "#   if x == 1:\n",
        "#     return True\n",
        "#   else:\n",
        "#     return False\n",
        "\n",
        "# results = filter(findOne, train_y)\n",
        "\n",
        "# for y in results:\n",
        "#   print(y)\n",
        "\n",
        "# from matplotlib import pyplot\n",
        "# for i in range(9):\n",
        "#   pyplot.subplot(330 + 1 + i)\n",
        "#   pyplot.imshow(train_X[i], cmap=pyplot.get_cmap('gray'))\n",
        "#   pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ko1w_hpHyQJ",
        "outputId": "7429024a-c008-48c5-c819-ae429622d3f5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.13768007 0.15553722 0.0972539  0.08570928 0.11611645 0.14806421\n",
            " 0.08826531 0.17940676 0.05439175 0.10956383 0.14279711 0.07127851\n",
            " 0.17887655 0.1422719  0.05813325 0.12653062 0.1232243  0.13558424\n",
            " 0.06797219 0.0887255  0.17087333 0.1772359  0.07828632 0.08291817\n",
            " 0.10988395 0.20398661 0.06792217 0.23065226 0.20842338 0.0787565\n",
            " 0.12756102 0.16289015 0.08794018 0.10591237 0.18260804 0.08166266\n",
            " 0.15939876 0.18640456 0.1097489  0.13445379 0.0685024  0.14158164\n",
            " 0.06320028 0.09005602 0.08600441 0.12083834 0.11595638 0.11229992\n",
            " 0.08596439 0.16487594 0.09379251 0.22780614 0.1397859  0.08405362\n",
            " 0.11799721 0.16189976 0.20954381 0.10035515 0.16983293 0.08422369\n",
            " 0.14733894 0.0907263  0.15162066 0.23131752 0.13140257 0.0829882\n",
            " 0.13494898 0.07610044 0.12360945 0.21144958 0.11048419 0.10207083\n",
            " 0.05379151 0.13060725 0.13078733 0.17024308 0.13146758 0.08484894\n",
            " 0.09869449 0.11668669 0.13094237 0.18483393 0.20099539 0.13401361\n",
            " 0.09305722 0.13032213 0.10920368 0.13967587 0.16943777 0.12593038\n",
            " 0.14892957 0.1297319  0.07691077 0.14667867 0.13641956 0.1886905\n",
            " 0.09080131 0.14642857 0.10759803 0.0642407 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yy = copy.deepcopy(train_y)\n",
        "unique_integers = list(set(yy))\n",
        "unique_integers.sort()\n",
        "print(unique_integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf_zG-h6H5y4",
        "outputId": "906ad2f3-09db-4aa5-ad23-4d6a11155020"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(number, num_classes):\n",
        "    one_hot_vector = [0] * num_classes\n",
        "    one_hot_vector[number] = 1\n",
        "    return one_hot_vector\n",
        "# convert train_y to one hot encoding\n",
        "num_classes = len(unique_integers)\n",
        "yy_one = [one_hot_encode(num, num_classes) for num in yy]\n",
        "print(yy_one[2])\n",
        "print(yy[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g0MtZ1FH877",
        "outputId": "88425f96-bd8b-4864-bb69-87ebfe633fcb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problematic MLP Code with Vanishing Gradient problem (Look below for solution)"
      ],
      "metadata": {
        "id": "Dclv4RXhIoW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(scores):\n",
        "    max_score = max(scores, key=lambda x: x.data)\n",
        "    norm_scores = [(score-max_score) for score in scores]\n",
        "    exp_scores = [norm_score.exp() for norm_score in norm_scores]\n",
        "    sum_exp_scores = sum(exp_scores)\n",
        "    return [exp_score / sum_exp_scores for exp_score in exp_scores]\n",
        "\n",
        "# define the MLP model\n",
        "in_inputs=28*28\n",
        "output_dim = len(unique_integers)\n",
        "model = MLP(in_inputs, [70,50,output_dim])\n",
        "\n",
        "# limit training set to overfit\n",
        "# on smaller test size.\n",
        "limit_x=5\n",
        "\n",
        "# reshape here to flatten the 2D [28,28] into 1D -> 28*28\n",
        "inputs = train_X[:limit_x].reshape(limit_x,-1)\n",
        "expected_outputs = yy_one[:limit_x]\n",
        "\n",
        "parameters_data_log = []\n",
        "parameters_grad_log = []\n",
        "# Begin gradient descent iterations\n",
        "iterations = 25\n",
        "for iter in range(iterations):\n",
        "  parameters_data_log.append([])\n",
        "  parameters_grad_log.append([])\n",
        "  # forward the model one input at a time to get scores\n",
        "  correct = 0\n",
        "  for i_input in range(len(inputs)):\n",
        "    scores = model(inputs[i_input])\n",
        "    # print('raw scores',scores)\n",
        "    probs_predicted = softmax(scores)\n",
        "    # Loss for a single input\n",
        "    yi_one = expected_outputs[i_input]\n",
        "    loss = []\n",
        "    probs_list = [p.data for p in probs_predicted]\n",
        "\n",
        "    if(yi_one.index(max(yi_one)) == probs_list.index(max(probs_list))):\n",
        "      correct += 1\n",
        "\n",
        "    #  Negative Log Likelihood loss\n",
        "    print('probs_predicted',probs_predicted)\n",
        "    for k in range(len(yi_one)):\n",
        "      loss.append(-1*((yi_one[k]*probs_predicted[k].log())+(1-yi_one[k])*(1-probs_predicted[k]).log()))\n",
        "    total_loss = sum(loss)/len(loss)\n",
        "\n",
        "    # print the probabilities at the last iteration\n",
        "    if iter == iterations-1:\n",
        "      print('probabilities predicted:',probs_list)\n",
        "      print('predicted value:',probs_list.index(max(probs_list)))\n",
        "      print('actual value:',yi_one.index(max(yi_one)))\n",
        "\n",
        "    # Back propagation\n",
        "    model.zero_grad()\n",
        "    total_loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    for p in model.parameters():\n",
        "      parameters_data_log[iter].append(p.data)\n",
        "      parameters_grad_log[iter].append(p.grad)\n",
        "      p.data -= p.grad\n",
        "  print('Iteration '+str(iter) +' total loss: '+str(total_loss.data))\n",
        "  print('Accuracy:'+str(correct/len(inputs)))"
      ],
      "metadata": {
        "id": "3yy8XOfLInwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Vanishing Gradient Problem In MLP solving MNIST\n",
        "- Fixes\n",
        "  - Using cross entropy loss instead of binary cross entropy loss because the outputs are mutually exclusive of each other\n",
        "  - The backwards() for log was incorrect. I was dividing by the output of log(x) and not 1/x itself.\n",
        "  - I still can not fix it. MNIST is too difficult for me to solve by myself. 09/14/24\n",
        "  - Okay I fixed it. I adjusted the learning_rate to be 0.05. But I think what did it is making the number of parameters and number of layers smaller.\n",
        "    - I went from 70,50 neurons in the deep layers to 7,5 then just 7 neurons.\n",
        "    - I took out a layer.\n",
        "    - Too many neurons may have contributed to the vanishing gradient problem."
      ],
      "metadata": {
        "id": "r-GrJkHtIGnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQuwVe6C-xXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a173e9b-455b-456d-cbd8-3b80e7620a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 Average loss across inputs: 7.254621218603589\n",
            "Accuracy:0.0\n",
            "Iteration 1 Average loss across inputs: 2.329408916780422\n",
            "Accuracy:0.4\n",
            "Iteration 2 Average loss across inputs: 0.6142281797161026\n",
            "Accuracy:1.0\n",
            "Iteration 3 Average loss across inputs: 0.5615841353104433\n",
            "Accuracy:1.0\n",
            "Iteration 4 Average loss across inputs: 0.5170907583270244\n",
            "Accuracy:1.0\n",
            "Iteration 5 Average loss across inputs: 0.4858859229038976\n",
            "Accuracy:1.0\n",
            "Iteration 6 Average loss across inputs: 0.4631256085707408\n",
            "Accuracy:1.0\n",
            "Iteration 7 Average loss across inputs: 0.4452594345213125\n",
            "Accuracy:1.0\n",
            "Iteration 8 Average loss across inputs: 0.4303040461830411\n",
            "Accuracy:1.0\n",
            "Iteration 9 Average loss across inputs: 0.41721617920584475\n",
            "Accuracy:1.0\n",
            "Iteration 10 Average loss across inputs: 0.40541555797040196\n",
            "Accuracy:1.0\n",
            "Iteration 11 Average loss across inputs: 0.39453076191533387\n",
            "Accuracy:1.0\n",
            "Iteration 12 Average loss across inputs: 0.3843018989945897\n",
            "Accuracy:1.0\n",
            "Iteration 13 Average loss across inputs: 0.3746165764885313\n",
            "Accuracy:1.0\n",
            "Iteration 14 Average loss across inputs: 0.36537199151737465\n",
            "Accuracy:1.0\n",
            "Iteration 15 Average loss across inputs: 0.356497064334212\n",
            "Accuracy:1.0\n",
            "Iteration 16 Average loss across inputs: 0.3479410393158087\n",
            "Accuracy:1.0\n",
            "Iteration 17 Average loss across inputs: 0.3396691962938777\n",
            "Accuracy:1.0\n",
            "Iteration 18 Average loss across inputs: 0.33165778096159343\n",
            "Accuracy:1.0\n",
            "probabilities predicted: [0.006263856499927867, 0.0030056087308796505, 0.00010693660020314391, 0.002593456385285058, 1.862114742288641e-05, 0.9879180063205425, 7.373517758974786e-05, 4.086172800287388e-08, 1.9261264828921414e-05, 4.770115921427734e-07]\n",
            "predicted value: 5\n",
            "actual value: 5\n",
            "probabilities predicted: [0.9842884101143146, 0.0001235086668050574, 3.367044546115914e-08, 6.607247147820148e-05, 1.4925820412742827e-09, 0.0069695132326846555, 0.0010981153704653984, 2.4298777446663767e-13, 2.385844139285298e-07, 0.007454106396567856]\n",
            "predicted value: 0\n",
            "actual value: 0\n",
            "probabilities predicted: [0.07919987361087656, 0.09111377346841228, 0.0875146173299542, 0.08577283237933908, 0.21153069108971959, 0.09056596651665733, 0.08382365027890096, 0.08763948702442102, 0.08574560879497128, 0.09709349950674767]\n",
            "predicted value: 4\n",
            "actual value: 4\n",
            "probabilities predicted: [0.00036669437321892054, 0.9874769748486694, 6.275199749277073e-07, 8.621212791676392e-07, 0.005410181715420485, 0.003203197830182445, 1.2853343504400528e-06, 1.8968466054071236e-06, 0.0012649097704221137, 0.002273369639876285]\n",
            "predicted value: 1\n",
            "actual value: 1\n",
            "probabilities predicted: [0.022108906933029016, 0.0008393141294289857, 1.1090757568868418e-05, 9.731614440747382e-06, 1.3877134993550839e-05, 0.00014584900215201948, 0.0006420301678918957, 4.5043293154342096e-07, 0.001271906815728296, 0.9749568430118352]\n",
            "predicted value: 9\n",
            "actual value: 9\n",
            "Iteration 19 Average loss across inputs: 0.32386825010301706\n",
            "Accuracy:1.0\n"
          ]
        }
      ],
      "source": [
        "# This will fix the vanishing gradient problem from Micrograd_MNIST Draft 3d.\n",
        "def softmax(scores):\n",
        "    max_score = max(scores, key=lambda x: x.data)\n",
        "    norm_scores = [(score-max_score) for score in scores]\n",
        "    exp_scores = [norm_score.exp() for norm_score in norm_scores]\n",
        "    sum_exp_scores = sum(exp_scores)\n",
        "    return [exp_score / sum_exp_scores for exp_score in exp_scores]\n",
        "\n",
        "# define the MLP model\n",
        "in_inputs=28*28\n",
        "output_dim = len(unique_integers)\n",
        "model = MLP(in_inputs, [7,output_dim])\n",
        "\n",
        "# limit training set to overfit\n",
        "# on smaller test size.\n",
        "limit_x=5\n",
        "\n",
        "# reshape here to flatten the 2D [28,28] into 1D -> 28*28\n",
        "inputs = train_X[:limit_x].reshape(limit_x,-1)\n",
        "expected_outputs = yy_one[:limit_x]\n",
        "\n",
        "parameters_data_log = []\n",
        "parameters_grad_log = []\n",
        "# Begin gradient descent iterations\n",
        "iterations = 20\n",
        "learning_rate = 0.05\n",
        "for iter in range(iterations):\n",
        "  parameters_data_log.append([])\n",
        "  parameters_grad_log.append([])\n",
        "  # forward the model one input at a time to get scores\n",
        "  correct = 0\n",
        "  total_loss = 0.0\n",
        "  for i_input in range(len(inputs)):\n",
        "    scores = model(inputs[i_input])\n",
        "    # print('raw scores',scores)\n",
        "    probs_predicted = softmax(scores)\n",
        "    # Loss for a single input\n",
        "    yi_one = expected_outputs[i_input]\n",
        "    # loss = []\n",
        "    probs_list = [p.data for p in probs_predicted]\n",
        "    correct_index = yi_one.index(max(yi_one))\n",
        "    if(correct_index == probs_list.index(max(probs_list))):\n",
        "      correct += 1\n",
        "\n",
        "    #  Negative Log Likelihood loss???\n",
        "    # Cross Entropy Loss\n",
        "    # print('probs_predicted',probs_predicted)\n",
        "    # for k in range(len(yi_one)):\n",
        "    # loss.append()\n",
        "    # print('probs_predicted[correct_index]',probs_predicted[correct_index])\n",
        "    loss = -1*(probs_predicted[correct_index]).log()\n",
        "    total_loss += loss.data\n",
        "\n",
        "    # print the probabilities at the last iteration\n",
        "    if iter == iterations-1:\n",
        "      print('probabilities predicted:',probs_list)\n",
        "      print('predicted value:',probs_list.index(max(probs_list)))\n",
        "      print('actual value:',yi_one.index(max(yi_one)))\n",
        "\n",
        "    # Back propagation\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    for p in model.parameters():\n",
        "      parameters_data_log[iter].append(p.data)\n",
        "      parameters_grad_log[iter].append(p.grad)\n",
        "      p.data -= p.grad * learning_rate\n",
        "  print('Iteration '+str(iter) +' Average loss across inputs: '+str(total_loss/len(inputs)))\n",
        "  print('Accuracy:'+str(correct/len(inputs)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('mean of gradients:',np.mean(parameters_grad_log, axis=1))\n",
        "print('mean of parameters:',np.mean(parameters_data_log, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e13n58QbpScc",
        "outputId": "5dcefe5d-d198-46ff-b3d9-524c48c08777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean of gradients: [ 0.01502411  0.00511081 -0.00109118 -0.00093783 -0.00074425 -0.00066191\n",
            " -0.00053507 -0.00044514 -0.00037884 -0.0003285  -0.00024089 -0.00025872\n",
            " -0.00023297 -0.00021173 -0.00019392 -0.00017879 -0.00016579 -0.00012555\n",
            " -0.00014497 -0.0001362 ]\n",
            "mean of parameters: [-0.00378242 -0.00668929 -0.0071486  -0.00688297 -0.00666677 -0.00649435\n",
            " -0.00633825 -0.006211   -0.00610454 -0.00601351 -0.00593428 -0.00587626\n",
            " -0.00581351 -0.00575686 -0.00570526 -0.00565791 -0.00561419 -0.00557358\n",
            " -0.00554288 -0.0055073 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run MLP on full MNIST dataset\n",
        "- TODOs:\n",
        "  - Implement batched gradient descent\n",
        "  - Do it efficiently by eliminating excessive for loops. (Pythonify your code)"
      ],
      "metadata": {
        "id": "-ZfHxhOSV1YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.permutation(5)\n",
        "np.array(yy_one).shape"
      ],
      "metadata": {
        "id": "OhtR8fddZ17u",
        "outputId": "d25ff1e2-e397-4d74-daa4-2b4a0da696b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(scores):\n",
        "    max_score = max(scores, key=lambda x: x.data)\n",
        "    norm_scores = [(score-max_score) for score in scores]\n",
        "    exp_scores = [norm_score.exp() for norm_score in norm_scores]\n",
        "    sum_exp_scores = sum(exp_scores)\n",
        "    return [exp_score / sum_exp_scores for exp_score in exp_scores]\n",
        "\n",
        "# define the MLP model\n",
        "in_inputs=28*28\n",
        "output_dim = len(unique_integers)\n",
        "model = MLP(in_inputs, [7,output_dim])\n",
        "\n",
        "# reshape here to flatten the 2D [28,28] into 1D -> 28*28\n",
        "inputs = train_X.reshape(-1,in_inputs)\n",
        "parameters_data_log = []\n",
        "parameters_grad_log = []\n",
        "# Begin gradient descent iterations\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 5\n",
        "learning_rate = 0.05\n",
        "batch_size=100\n",
        "n = len(inputs)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  parameters_data_log.append([])\n",
        "  parameters_grad_log.append([])\n",
        "  # forward the model with randomly selected batches\n",
        "  indices = np.random.permutation(n)\n",
        "  correct = 0\n",
        "  total_loss = 0.0\n",
        "  X_shuffled = inputs[indices]\n",
        "  y_shuffled = yy_one[indices]\n",
        "  for i in range(0, n, batch_size):\n",
        "    X_batch = X_shuffled[i:i + batch_size]\n",
        "    y_batch = y_shuffled[i:i + batch_size]\n",
        "    # Forward propagation\n",
        "    logits = model(X_batch)\n",
        "    probs = [softmax(logits) for logit in logits]\n",
        "    # probs_list = [p.data for prob in probs for p in prob]\n",
        "    # correct_index = yi_one.index(max(yi_one))\n",
        "    # if(correct_index == probs_list.index(max(probs_list))):\n",
        "    #   correct += 1\n",
        "  #   # Cross Entropy Loss\n",
        "  #   loss = -1*(probs[correct_index]).log()\n",
        "  #   total_loss += loss.data\n",
        "  #   # Back propagation\n",
        "  #   model.zero_grad()\n",
        "  #   loss.backward()\n",
        "  #   # Update parameters\n",
        "  #   for p in model.parameters():\n",
        "  #     parameters_data_log[epoch].append(p.data)\n",
        "  #     parameters_grad_log[epoch].append(p.grad)\n",
        "  #     p.data -= p.grad * learning_rate\n",
        "  #   # print the probabilities at the last iteration\n",
        "  #   if epoch == epochs-1:\n",
        "  #     print('probabilities predicted:',probs_list)\n",
        "  #     print('predicted value:',probs_list.index(max(probs_list)))\n",
        "  #     print('actual value:',yi_one.index(max(yi_one)))\n",
        "  # print(f'Epoch {epoch+1}, Loss: {total_loss/len(inputs)}, Accuracy: {correct/len(inputs)}')"
      ],
      "metadata": {
        "id": "agA6G0cOV0sI",
        "outputId": "a7913fcf-6e37-4bb3-b74a-41b0399432c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "only integer scalar arrays can be converted to a scalar index",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c16e8d30efe0>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mX_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0my_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myy_one\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    }
  ]
}